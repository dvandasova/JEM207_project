{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing home-made libraries\n",
    "from app.scraping import scrape_accommodation_data\n",
    "from app.scraping import scrape_min_price\n",
    "from app.scraping import start_webdriver\n",
    "from app.scraping import close_webdriver\n",
    "\n",
    "from app.geos import get_coordinates\n",
    "\n",
    "from app.bundling import select_accommodation_bundles\n",
    "\n",
    "from app.input import read_excel_to_df\n",
    "from app.input import check_code\n",
    "from app.input import read_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for geo locations\n",
    "from geopy.geocoders import Nominatim # Importing the geopy library and Nominatim class\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from geopy.distance import Distance\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for web scraping\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from itertools import product\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from datetime import timedelta\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the \"full dataset\" (shortened for convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing internal match data\n",
    "url = \"https://raw.githubusercontent.com/dvandasova/JEM207_project/b5414c0b6fd52309880b8478913089768dbe320e/02_Datasets/internal-data.xlsx\"\n",
    "response = requests.get(url)\n",
    "df = pd.read_excel(BytesIO(response.content))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omitting all matches within 14 days to prevent cases with no fligts or accomodation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['date'] >= (datetime.now() + timedelta(days=14))]\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the imported scraping functions, first scrape flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row in the DataFrame\n",
    "min_prices = []  # To store the minimum prices for each city and date combination\n",
    "for index, row in df.iterrows():\n",
    "    city = row['flight.Code']  # Use the column name 'flight.Code' for the city\n",
    "    departure_date = row['date'].strftime('%Y-%m-%d')  # Assuming 'date' is in a proper date format\n",
    "\n",
    "    # Calculate the return date as departure date + 2 days\n",
    "    return_date = (row['date'] + timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Scrape the minimum price for the city and date combination\n",
    "    min_price = scrape_min_price(city, departure_date, return_date)\n",
    "    \n",
    "    # Append the minimum price to the list\n",
    "    min_prices.append(min_price)\n",
    "\n",
    "    print(min_price)\n",
    "\n",
    "# Add the minimum price as a new column in the original DataFrame\n",
    "df['Min Price'] = min_prices\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now scrape the accomodations and create the bundles with each scraping iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists to hold the accommodation data for each row\n",
    "accommodation_standard = []\n",
    "accommodation_superior = []\n",
    "accommodation_luxurious = []\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Loop through each row in the DataFrame to get location and dates\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    location = row['accom.Code']  # Assuming 'accom.Code' contains the location identifier\n",
    "    departure_date = row['date'].strftime('%Y-%m-%d')  # Assuming 'date' is a date column\n",
    "    return_date = (row['date'] + timedelta(days=2)).strftime('%Y-%m-%d') \n",
    "\n",
    "    city = row['city']  # Assuming 'city' contains the city name\n",
    "    \n",
    "    # Scrape accommodation data\n",
    "    accommodation_data = scrape_accommodation_data(location, departure_date, return_date)\n",
    "    accommodation_data[\"Location\"] = accommodation_data[\"Location\"] + \", \" + city\n",
    "    \n",
    "    # Filter out locations with \"Nearby - \"\n",
    "    accommodation_data = accommodation_data[~accommodation_data['Location'].str.contains(\"Nearby - \", na=False)]\n",
    "\n",
    "    current_venue = row[\"venue\"] \n",
    "\n",
    "    # Retrieve the coordinates for each accommodation\n",
    "    places = get_coordinates(accommodation_data['Location'])\n",
    "\n",
    "    # Create a DataFrame for places and coordinates\n",
    "    places_df = pd.DataFrame(places, columns=[\"Place\", \"Latitude\", \"Longitude\"])\n",
    "\n",
    "    # Geocode the current venue\n",
    "    geolocator = Nominatim(user_agent=\"Geopy Library\")\n",
    "    current_venue_location = geolocator.geocode(current_venue)\n",
    "\n",
    "    # Check if venue location was found\n",
    "    if current_venue_location:\n",
    "        base_coords = (current_venue_location.latitude, current_venue_location.longitude)\n",
    "    else:\n",
    "        base_coords = (\"Location not found\", \"Location not found\")\n",
    "\n",
    "    base_latitude = base_coords[0]\n",
    "    base_longitude = base_coords[1]\n",
    "\n",
    "    # Iterate over the places to replace \"Location not found\" with the base coordinates\n",
    "    for i, row in places_df.iterrows():\n",
    "        if row[\"Latitude\"] == \"Location not found\":\n",
    "            places_df.at[i, \"Latitude\"] = base_latitude\n",
    "            places_df.at[i, \"Longitude\"] = base_longitude\n",
    "            places_df.at[i, \"Latitude\"] = places_df.at[i, \"Latitude\"] + 0.1 \n",
    "            places_df.at[i, \"Longitude\"] = places_df.at[i, \"Longitude\"] + 0.1 \n",
    "\n",
    "\n",
    "\n",
    "    # Now places_df contains both the valid coordinates and venue coordinates for \"Location not found\"\n",
    "    # Combine this updated data back into the accommodation_data\n",
    "    accommodation_data = accommodation_data.reset_index(drop=True)\n",
    "\n",
    "    # Add latitude and longitude from places_df to accommodation_data\n",
    "    accommodation_data[\"Latitude\"] = places_df[\"Latitude\"]\n",
    "    accommodation_data[\"Longitude\"] = places_df[\"Longitude\"]\n",
    "\n",
    "    # Calculate distances between each accommodation and the venue\n",
    "    distances = []\n",
    "    for i in range(len(places_df)):\n",
    "        latitude = places_df.iloc[i][\"Latitude\"]\n",
    "        longitude = places_df.iloc[i][\"Longitude\"]\n",
    "\n",
    "        # Only calculate distance if both latitude and longitude are valid\n",
    "        if base_latitude != \"Location not found\" and latitude != \"Location not found\":\n",
    "            dist = geodesic((base_latitude, base_longitude), (latitude, longitude)).kilometers\n",
    "        else:\n",
    "            dist = float('inf')  # Assign a large distance if location is invalid\n",
    "\n",
    "        distances.append(dist)\n",
    "\n",
    "    # Add distances to accommodation_data\n",
    "    accommodation_data[\"Distance\"] = distances\n",
    "\n",
    "    # Print the accommodation data (for debugging purposes)\n",
    "    print(accommodation_data)\n",
    "\n",
    "    # Select the Standard, Superior, and Luxurious bundles\n",
    "    Standard_bundle, Superior_bundle, Luxurious_bundle = select_accommodation_bundles(accommodation_data)\n",
    "\n",
    "    # Append the accommodation names and prices to the corresponding lists\n",
    "    accommodation_standard.append({\n",
    "        'Name': Standard_bundle['Name'],\n",
    "        'Price': Standard_bundle['Price'],\n",
    "        'Rating': Standard_bundle['Rating']\n",
    "    })\n",
    "    accommodation_superior.append({\n",
    "        'Name': Superior_bundle['Name'],\n",
    "        'Price': Superior_bundle['Price'],\n",
    "        'Rating': Superior_bundle['Rating']\n",
    "    })\n",
    "    accommodation_luxurious.append({\n",
    "        'Name': Luxurious_bundle['Name'],\n",
    "        'Price': Luxurious_bundle['Price'],\n",
    "        'Rating': Luxurious_bundle['Rating']\n",
    "    })\n",
    "\n",
    "# Add the accommodation bundles to the original DataFrame\n",
    "df['Standard Accommodation'] = [x['Name'] for x in accommodation_standard]\n",
    "df['Superior Accommodation'] = [x['Name'] for x in accommodation_superior]\n",
    "df['Luxurious Accommodation'] = [x['Name'] for x in accommodation_luxurious]\n",
    "\n",
    "df['Standard Price'] = [x['Price'] for x in accommodation_standard]\n",
    "df['Superior Price'] = [x['Price'] for x in accommodation_superior]\n",
    "df['Luxurious Price'] = [x['Price'] for x in accommodation_luxurious]\n",
    "\n",
    "df['Standard Rating'] = [x['Rating'] for x in accommodation_standard]\n",
    "df['Superior Rating'] = [x['Rating'] for x in accommodation_superior]\n",
    "df['Luxurious Rating'] = [x['Rating'] for x in accommodation_luxurious]\n",
    "\n",
    "# Save the updated DataFrame with accommodation data\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the prices are numeric for easier further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Min Price' to numeric, treating 'n/a' and other non-numeric entries as 0\n",
    "df['Min Price'] = pd.to_numeric(df['Min Price'], errors='coerce').fillna(0)\n",
    "\n",
    "df['Standard Price'] = df['Standard Price'].replace({'\\$': '', ',': ''}, regex=True)\n",
    "df['Standard Price'] = pd.to_numeric(df['Standard Price'], errors='coerce')\n",
    "\n",
    "df['Superior Price'] = df['Superior Price'].replace({'\\$': '', ',': ''}, regex=True)\n",
    "df['Superior Price'] = pd.to_numeric(df['Superior Price'], errors='coerce')\n",
    "\n",
    "df['Luxurious Price'] = df['Luxurious Price'].replace({'\\$': '', ',': ''}, regex=True)\n",
    "df['Luxurious Price'] = pd.to_numeric(df['Luxurious Price'], errors='coerce')\n",
    "\n",
    "# Now perform the summation\n",
    "df[\"Standard Price Total\"] = pd.to_numeric(df[\"Standard Price\"], errors='coerce').fillna(0) + df[\"Min Price\"]\n",
    "df[\"Superior Price Total\"] = pd.to_numeric(df[\"Superior Price\"], errors='coerce').fillna(0) + df[\"Min Price\"]\n",
    "df[\"Luxurious Price Total\"] = pd.to_numeric(df[\"Luxurious Price\"], errors='coerce').fillna(0) + df[\"Min Price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the final scraped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset, or further use the results (vizualize etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('your_file.xlsx', engine='xlsxwriter', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"your_download_path\"  # Change this to your desired path\n",
    "df.to_excel(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
